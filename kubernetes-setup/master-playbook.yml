---
- name: Configure Kubernetes master
  hosts: localhost
  become: true
  vars:
    k8s_version: "1.28.15-1.1"
    pod_network_cidr: "10.244.0.0/16"
    control_plane_endpoint: "192.168.56.10"

  tasks:
    # System Configuration
    - name: Disable swap
      shell: |
        swapoff -a
        [ -f /etc/fstab ] && sed -i '/ swap / s/^/#/' /etc/fstab
      args:
        warn: false

    - name: Configure sysctl parameters
      shell: |
        cat <<EOF | tee /etc/sysctl.d/k8s.conf
        net.ipv4.ip_forward = 1
        net.bridge.bridge-nf-call-ip6tables = 1
        net.bridge.bridge-nf-call-iptables = 1
        EOF
        sysctl --system
      args:
        warn: false

    # Containerd Installation
    - name: Install containerd
      apt:
        name: containerd
        state: present
        update_cache: yes

    - name: Create containerd directories
      file:
        path: "{{ item }}"
        state: directory
        mode: 0755
      loop:
        - /etc/containerd
        - /run/containerd
        - /var/lib/containerd

    - name: Configure containerd
      copy:
        dest: /etc/containerd/config.toml
        content: |
          version = 2
          [plugins]
            [plugins."io.containerd.grpc.v1.cri"]
              sandbox_image = "registry.k8s.io/pause:3.8"
              [plugins."io.containerd.grpc.v1.cri".containerd]
                snapshotter = "overlayfs"
                [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]
                  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
                    runtime_type = "io.containerd.runc.v2"
                    [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
                      SystemdCgroup = false

    - name: Configure containerd service (systemd)
      block:
        - name: Create systemd drop-in
          copy:
            dest: /etc/systemd/system/containerd.service.d/override.conf
            content: |
              [Service]
              ExecStart=
              ExecStart=/usr/bin/containerd --config /etc/containerd/config.toml
          
        - name: Start and enable containerd
          systemd:
            name: containerd
            state: started
            enabled: yes
            daemon_reload: yes
      when: ansible_service_mgr == "systemd"

    - name: Start containerd (non-systemd)
      shell: |
        pkill containerd || true
        rm -f /run/containerd/containerd.sock
        nohup /usr/bin/containerd --config /etc/containerd/config.toml >/var/log/containerd.log 2>&1 &
        timeout 30 bash -c 'until [ -S /run/containerd/containerd.sock ]; do sleep 1; done'
      args:
        warn: false
      when: ansible_service_mgr != "systemd"

    - name: Verify containerd
      block:
        - name: Check containerd socket
          wait_for:
            path: /run/containerd/containerd.sock
            timeout: 30
        
        - name: Test containerd connectivity
          command: ctr version
          register: ctr_test
          changed_when: false
          retries: 3
          delay: 5
          until: ctr_test.rc == 0
        
        - name: Fail if containerd not working
          fail:
            msg: |
              Containerd failed to start properly.
              Check logs with: journalctl -u containerd || cat /var/log/containerd.log
              Socket status: ls -la /run/containerd/containerd.sock
          when: ctr_test.rc != 0

    # Kubernetes Installation
    - name: Install prerequisites
      apt:
        name:
          - apt-transport-https
          - ca-certificates
          - curl
          - gnupg
          - software-properties-common
        state: present
        update_cache: yes

    - name: Setup Kubernetes repository
      block:
        - name: Create keyrings directory
          file:
            path: /usr/share/keyrings
            state: directory
            mode: 0755

        - name: Add Kubernetes GPG key
          shell: |
            curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | gpg --dearmor -o /usr/share/keyrings/kubernetes-archive-keyring.gpg
            chmod 644 /usr/share/keyrings/kubernetes-archive-keyring.gpg
          args:
            warn: false

        - name: Add Kubernetes repo
          apt_repository:
            repo: "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /"
            state: present
            filename: kubernetes
            mode: 0644

    - name: Install Kubernetes packages
      block:
        - name: Update apt cache
          apt:
            update_cache: yes
          register: apt_update
          until: apt_update is succeeded
          retries: 3
          delay: 5

        - name: Install kube components
          apt:
            name:
              - kubelet={{ k8s_version }}
              - kubeadm={{ k8s_version }}
              - kubectl={{ k8s_version }}
            state: present
        
        - name: Hold Kubernetes packages
          command: apt-mark hold kubelet kubeadm kubectl

- name: Stop Kubernetes services
  block:
    - name: Attempt to stop Kubernetes processes
      shell: |
        # First try graceful shutdown
        pkill -f 'kubelet|kube-apiserver|kube-scheduler|kube-controller-manager|etcd' || true
        sleep 5
        
        # Force kill if still running
        pkill -9 -f 'kubelet|kube-apiserver|kube-scheduler|kube-controller-manager|etcd' || true
        sleep 2
        
        # Handle kubelet PID file if exists
        [ -f /var/run/kubelet.pid ] && { kill -9 $(cat /var/run/kubelet.pid); rm -f /var/run/kubelet.pid; } || true
        
        # Return success regardless of pkill outcome
        exit 0
      args:
        warn: false
      register: stop_services
      changed_when: stop_services.rc == 0
      ignore_errors: true

    - name: Verify processes are stopped
      shell: |
        ! pgrep -f 'kubelet|kube-apiserver|kube-scheduler|kube-controller-manager|etcd'
      register: verify_stopped
      changed_when: false
      failed_when: verify_stopped.rc != 0
      retries: 3
      delay: 5

  rescue:
    - name: Force cleanup if stopping failed
      shell: |
        # Nuclear option - stop all containers
        { command -v crictl >/dev/null && crictl rm -fa; } || true
        { command -v crictl >/dev/null && crictl rmp -fa; } || true
        
        # Force remove files
        find /etc/kubernetes -mindepth 1 -delete || true
        find /var/lib/kubelet -mindepth 1 -not -name '.kubelet-keep' -delete || true
        
        # Clean network
        iptables -F && iptables -t nat -F && iptables -t mangle -F || true
        ipvsadm --clear || true
      args:
        warn: false
      when: verify_stopped is failed

    - name: Reboot if necessary
      command: reboot
      async: 60
      poll: 0
      when: verify_stopped is failed
      ignore_errors: true

    - name: Fail with cleanup instructions
      fail:
        msg: |
          Kubernetes services could not be stopped cleanly.
          The system has attempted forced cleanup and may need reboot.
          Manual steps:
          1. Reboot the node: 'sudo reboot'
          2. Verify no processes: 'pgrep -f "kube|etcd" || echo "Clean"'
          3. Delete directories:
             'sudo rm -rf /etc/kubernetes/*'
             'sudo find /var/lib/kubelet -mindepth 1 -not -name ".kubelet-keep" -delete'
      when: verify_stopped is failed
    - name: Initialize Kubernetes cluster
      block:
        - name: Verify clean environment
          shell: |
            [ ! -f /etc/kubernetes/manifests/kube-apiserver.yaml ] && \
            [ ! -f /etc/kubernetes/manifests/kube-controller-manager.yaml ] && \
            echo "Environment clean" || exit 1
          changed_when: false

        - name: Run kubeadm init
          shell: |
            set -o pipefail
            kubeadm init \
              --kubernetes-version=v{{ k8s_version.split('-')[0] }} \
              --pod-network-cidr={{ pod_network_cidr }} \
              --ignore-preflight-errors=Swap \
              --cri-socket=unix:///run/containerd/containerd.sock \
              --control-plane-endpoint={{ control_plane_endpoint }} \
              --image-repository=registry.k8s.io \
              --upload-certs \
              --skip-phases=addon/kube-proxy 2>&1 | tee /var/log/kubeadm-init.log
          register: init_result
          retries: 2
          delay: 10

      rescue:
        - name: Show initialization logs
          debug:
            msg: |
              Cluster initialization failed!
              Logs:
              {{ lookup('file', '/var/log/kubeadm-init.log') | default('No logs found') }}
              Suggested actions:
              1. Check containerd status
              2. Verify network connectivity
              3. Check system resources

    - name: Configure kubectl
      block:
        - name: Setup kubeconfig
          shell: |
            mkdir -p $HOME/.kube
            cp -f /etc/kubernetes/admin.conf $HOME/.kube/config
            chmod 600 $HOME/.kube/config

        - name: Verify cluster status
          command: kubectl get nodes
          register: cluster_status
          retries: 3
          delay: 5
          until: cluster_status.rc == 0

  handlers:
    - name: restart containerd
      systemd:
        name: containerd
        state: restarted
